{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Env"
      ],
      "metadata": {
        "id": "odbsOwinCrlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Gym"
      ],
      "metadata": {
        "id": "Jq_l-L-ZCtr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install gymnasium"
      ],
      "metadata": {
        "id": "w72_n0tqCwUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install stable_baselines3"
      ],
      "metadata": {
        "id": "EkaWljUfDBHe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "# reward function parameters\n",
        "PERSISTENCE_PENALTY = 0\n",
        "HIT_REWARD = 1\n",
        "REPEATED_PENALTY = -0.2\n",
        "RADIUS = 2\n",
        "PROXIMAL_REWARD = 0.2\n",
        "SCORE_REWARD = 10"
      ],
      "metadata": {
        "id": "xGANo3tfDuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxbfJgu9CIBp"
      },
      "outputs": [],
      "source": [
        "class BattleshipEnv(gym.Env):\n",
        "    def __init__(self, board_size=10, ship_sizes=[5, 4, 3, 3, 2]):\n",
        "        super(BattleshipEnv, self).__init__()\n",
        "        self.board_size = board_size\n",
        "        self.ship_sizes = ship_sizes\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(board_size, board_size), dtype=np.int8)\n",
        "        self.action_space = spaces.Discrete(board_size * board_size)  # Attack grid positions\n",
        "        self.seed_value = None\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.hits = []\n",
        "        self.steps_taken = []\n",
        "\n",
        "        # Initialize board and randomly place ships with dtype=int8 to match observation_space\n",
        "        self.ship_board = np.zeros((self.board_size, self.board_size), dtype=np.int8)  # 0: water, 1: ship\n",
        "        self._place_ships()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Set the seed if provided\n",
        "        self.seed_value = seed\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        #self.action_space = np.arange(100)  # Attack grid positions\n",
        "        self.board = np.full((self.board_size, self.board_size), -1, dtype=np.int8)  # -1: unknown, 0: miss, 1: hit\n",
        "        self.steps_taken.append(self.current_step)\n",
        "\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.hits = []\n",
        "        return self.board, {}\n",
        "\n",
        "    def _place_ships(self):\n",
        "        for ship_size in self.ship_sizes:\n",
        "            placed = False\n",
        "            while not placed:\n",
        "                row, col = np.random.randint(0, self.board_size), np.random.randint(0, self.board_size)\n",
        "                orientation = np.random.choice(['horizontal', 'vertical'])\n",
        "                if self._can_place_ship(row, col, ship_size, orientation):\n",
        "                    self._place_ship(row, col, ship_size, orientation)\n",
        "                    placed = True\n",
        "\n",
        "    def _can_place_ship(self, row, col, ship_size, orientation):\n",
        "        if orientation == 'horizontal':\n",
        "            if col + ship_size > self.board_size:\n",
        "                return False\n",
        "            return np.all(self.ship_board[row, col:col+ship_size] == 0)\n",
        "        else:  # vertical\n",
        "            if row + ship_size > self.board_size:\n",
        "                return False\n",
        "            return np.all(self.ship_board[row:row+ship_size, col] == 0)\n",
        "\n",
        "    def _place_ship(self, row, col, ship_size, orientation):\n",
        "        if orientation == 'horizontal':\n",
        "            self.ship_board[row, col:col+ship_size] = 1\n",
        "        else:\n",
        "            self.ship_board[row:row+ship_size, col] = 1\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.board_size)\n",
        "        reward = 0\n",
        "\n",
        "        if self.board[row,col] == 0 or self.board[row,col] == 1:\n",
        "            reward = REPEATED_PENALTY\n",
        "\n",
        "            terminated = len(self.hits) == sum(self.ship_sizes)  # Game over when all ships are hit\n",
        "            truncated = self.current_step > 2 * self.board_size ** 2  # End if the game takes too long\n",
        "\n",
        "            return self.board, reward, terminated, truncated, {}\n",
        "\n",
        "        # Determine hit or miss\n",
        "        elif self.ship_board[row, col] == 1:\n",
        "            self.board[row, col] = 1  # Mark as hit\n",
        "            reward = HIT_REWARD  # Bonus for consecutive h\n",
        "\n",
        "            self.hits.append((row,col))\n",
        "\n",
        "        else:\n",
        "            self.concurrent_hits = False\n",
        "            self.board[row, col] = 0  # Mark as miss\n",
        "\n",
        "            reward = PERSISTENCE_PENALTY # Increase penalty with consecutive misses\n",
        "\n",
        "\n",
        "        p_rewards = self._checkDistance((row,col), self.hits)\n",
        "        reward += p_rewards\n",
        "        # Check termination conditions\n",
        "        terminated = len(self.hits) == sum(self.ship_sizes)  # Game over when all ships are hit\n",
        "        truncated = self.current_step > 2 * self.board_size ** 2  # End if the game takes too long\n",
        "\n",
        "        if terminated:\n",
        "            reward += SCORE_REWARD\n",
        "\n",
        "        return self.board, reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(\"Board:\\n\", self.board)\n",
        "\n",
        "    def _checkDistance(self,p1, hits):\n",
        "        rewards = 0\n",
        "        for hit in hits:\n",
        "            x1, y1 = p1\n",
        "            x2, y2 = hit\n",
        "            d = abs(x2-x1) + abs(y2-y1)\n",
        "            if d <= RADIUS:\n",
        "                rewards += PROXIMAL_REWARD\n",
        "\n",
        "        return rewards\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "mQpaqjzAC2ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import stable_baselines3"
      ],
      "metadata": {
        "id": "37L5NWWVC-ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9s8KnCuxLdhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from stable_baselines3 import DQN, PPO, A2C\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class RewardLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(RewardLoggerCallback, self).__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.episode_reward = 0\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Accumulate reward for the current episode\n",
        "        self.episode_reward += self.locals[\"rewards\"][0]\n",
        "\n",
        "        # Check if the episode is done\n",
        "        if self.locals[\"dones\"][0]:\n",
        "            # Log episode reward and reset\n",
        "            self.episode_rewards.append(self.episode_reward)\n",
        "            if self.verbose > 0:\n",
        "                #print(f\"Episode {len(self.episode_rewards)} reward: {self.episode_reward}\")\n",
        "                pass\n",
        "            self.episode_reward = 0\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "            # Print all rewards collected at the end\n",
        "        print(\"Training finished.\")\n",
        "        print(\"Rewards per episode:\", self.episode_rewards)\n",
        "\n",
        "class StepLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(StepLoggerCallback, self).__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_reward = 0\n",
        "        self.current_steps = 0\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Accumulate reward and step count for the current episode\n",
        "        self.episode_reward += self.locals[\"rewards\"][0]\n",
        "        self.current_steps += 1\n",
        "\n",
        "        # Check if the episode is done\n",
        "        if self.locals[\"dones\"][0]:\n",
        "            # Log episode reward and steps, then reset\n",
        "            self.episode_rewards.append(self.episode_reward)\n",
        "            self.episode_steps.append(self.current_steps)\n",
        "            if self.verbose > 0:\n",
        "              #print(f\"Episode {len(self.episode_rewards)}: Reward = {self.episode_reward}, Steps = {self.current_steps}\")\n",
        "              pass\n",
        "            # Reset counters\n",
        "            self.episode_reward = 0\n",
        "            self.current_steps = 0\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        # Print rewards and steps per episode at the end of training\n",
        "        print(\"Training finished.\")\n",
        "        print(\"Rewards per episode:\", self.episode_rewards)\n",
        "        print(\"Steps per episode:\", self.episode_steps)\n",
        "\n"
      ],
      "metadata": {
        "id": "nmVu6Bc6C3v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment\n",
        "env = BattleshipEnv(board_size=10)\n",
        "env = Monitor(env)\n",
        "\n",
        "# Initialize the DQN model\n",
        "model = PPO('MlpPolicy', env, verbose=0)  # 'MlpPolicy' uses a fully connected neural network by default\n",
        "\n",
        "reward_callback = RewardLoggerCallback(verbose=1)\n",
        "step_callback = StepLoggerCallback(verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10_000, callback=step_callback)  # You can adjust the number of timesteps\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the steps per episode\n",
        "plt.plot(step_callback.episode_steps)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Steps Taken')\n",
        "plt.title('Steps Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qQ_UKLQsToTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the steps per episode\n",
        "plt.plot(env.get_episode_rewards())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Rewards Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XBSGVyAMMizL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the steps per episode\n",
        "plt.plot(env.get_episode_lengths())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Steps Taken')\n",
        "plt.title('Steps Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JMzQZjmYPLEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the environment\n",
        "env2 = BattleshipEnv(board_size=10)\n",
        "env2 = Monitor(env2)\n",
        "\n",
        "# Initialize the DQN model\n",
        "model2 = DQN('MlpPolicy', env2, verbose=0)  # 'MlpPolicy' uses a fully connected neural network by default\n",
        "\n",
        "# reward_callback = RewardLoggerCallback(verbose=0)\n",
        "# step_callback = StepLoggerCallback(verbose=0)\n",
        "\n",
        "# Train the model\n",
        "model2.learn(total_timesteps=100_000)  # You can adjust the number of timesteps"
      ],
      "metadata": {
        "id": "8tOHPJ0zPTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the steps per episode\n",
        "plt.plot(env2.get_episode_rewards())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Rewards Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2wvBINPTRbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the steps per episode\n",
        "plt.plot(env2.get_episode_lengths())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Steps Taken')\n",
        "plt.title('Steps Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgn5EZ59TZqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the environment\n",
        "env3 = BattleshipEnv(board_size=10)\n",
        "env3 = Monitor(env3)\n",
        "\n",
        "# Initialize the DQN model\n",
        "model3 = A2C('MlpPolicy', env3, verbose=1)  # 'MlpPolicy' uses a fully connected neural network by default\n",
        "\n",
        "# reward_callback = RewardLoggerCallback(verbose=0)\n",
        "# step_callback = StepLoggerCallback(verbose=0)\n",
        "\n",
        "# Train the model\n",
        "model3.learn(total_timesteps=100_000)  # You can adjust the number of timesteps"
      ],
      "metadata": {
        "id": "2KGIh_1CUgTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(env3.get_episode_rewards())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Rewards Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c207H42ngUHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the steps per episode\n",
        "plt.plot(env3.get_episode_lengths())\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Steps Taken')\n",
        "plt.title('Steps Taken per Episode During Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ob0J9bF5gU9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ShowGraphs(steps, rewards):\n",
        "  # Create a figure with 1 row and 2 columns\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # Adjust figsize as needed\n",
        "\n",
        "  # Plot Steps Taken per Episode on the first subplot\n",
        "  ax1.plot(steps)\n",
        "  ax1.set_xlabel('Episode')\n",
        "  ax1.set_ylabel('Steps Taken')\n",
        "  ax1.set_title('Steps Taken per Episode During Evaluation')\n",
        "\n",
        "  # Plot Rewards per Episode on the second subplot\n",
        "  ax2.plot(rewards)\n",
        "  ax2.set_xlabel('Episode')\n",
        "  ax2.set_ylabel('Rewards')\n",
        "  ax2.set_title('Rewards per Episode During Evaluation')\n",
        "\n",
        "  # Display the plots\n",
        "  plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "0_g8aL9vijW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateModel(model, num_episodes, env):\n",
        "  steps_ep = []\n",
        "  reward_ep = []\n",
        "  for i in range(num_episodes):\n",
        "    steps = 0\n",
        "    t_reward = 0\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    while True:\n",
        "      action, _states = model.predict(obs)\n",
        "      new_obs, reward, done, truncated, info = env.step(action)\n",
        "      t_reward+=reward\n",
        "\n",
        "      obs = new_obs\n",
        "\n",
        "      if done or truncated:\n",
        "        steps_ep.append(steps)\n",
        "        reward_ep.append(t_reward)\n",
        "        break\n",
        "\n",
        "      steps+=1\n",
        "  ShowGraphs(steps_ep, reward_ep)\n",
        "  return steps_ep, reward_ep\n"
      ],
      "metadata": {
        "id": "8Mgj5NofgagC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _ = evaluateModel(model, 100, BattleshipEnv(board_size=10))"
      ],
      "metadata": {
        "id": "Td8T3bj4iOw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _ = evaluateModel(model2, 100, BattleshipEnv(board_size=10))"
      ],
      "metadata": {
        "id": "_c8nr1PviSRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _ = evaluateModel(model3, 100, BattleshipEnv(board_size=10))"
      ],
      "metadata": {
        "id": "wwRbTfLeuwTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7T2gF0Uux6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}